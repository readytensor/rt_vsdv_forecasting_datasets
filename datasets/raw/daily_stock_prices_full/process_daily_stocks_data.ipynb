{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Daily Stock Prices Dataset\n",
    "\n",
    "This dataset provides a diverse representation of historical stock prices from selected S&P 500 companies over the last three decades. It has been curated with the aim to capture the underlying trends, patterns, and fluctuations of individual stock prices, independent of broader market influences. The dataset offers a rich blend of leading companies from the S&P 500 list of 1990 combined with a random selection from the remainder of the list. The dataset contains daily stock data spanning 1000 days (roughly equivalent to 4 years) for each of the 52 selected stocks. In a step towards ensuring strict technical analysis, the stock tickers have been replaced with randomly generated unique strings to mask the original tickers and remove any potential analyst bias.\n",
    "\n",
    "### Dataset Composition:\n",
    "\n",
    "1. **Historical Reference:** The dataset includes the top 10 companies from the 1990 S&P 500 list that are still in existence, supplemented by a random selection from the remaining companies, ensuring all companies included are currently operational.\n",
    "\n",
    "2. **Random Time Window:** A random start date for each company's data was selected from between the beginning of 1990 to the beginning of 2016. From these start dates, 1000 consecutive trading days of data were gathered, amounting to roughly four years of trading data for each stock.\n",
    "\n",
    "3. **De-correlation Objective:** The choice of random start dates for each stock dataset prevents the ensemble from reflecting a uniform market period, avoiding highly correlated stock movements. This method aims to offer a more decorrelated and realistic picture of each stock's performance, irrespective of market trends.\n",
    "\n",
    "4. **Anonymization of Identifiers:** To focus the analysis purely on price movements and technical indicators, company identifiers have been replaced with anonymized strings, creating a level playing field for technical analysis without preconceived biases.\n",
    "\n",
    "5. **Dataset Structure:** The final saved dataset is structured as a CSV file, with columns for the anonymized ticker, the epoch (day number from 1 to 1000), and various stock price fields such as 'Adj Close', 'Close', 'High', 'Low', 'Open', and 'Volume'.\n",
    "\n",
    "### Advantages of the Dataset:\n",
    "\n",
    "- **Diverse Temporal Insights:** The dataset spans various market conditions, offering insights into stock behavior during different economic cycles, including bull markets and recessions.\n",
    "\n",
    "- **Time Series Forecasting:** With its temporal spread and de-correlation strategy, the dataset serves as an ideal benchmark for time series forecasting models that aim to predict stock price movements.\n",
    "\n",
    "- **Technical Analysis Focus:** The anonymization of stock tickers shifts the focus entirely to the technical aspects of the stock data, making it a robust resource for analysts practicing technical analysis without influence from the companies' fundamental data.\n",
    "\n",
    "The **S&P 500 Historical Sampler Dataset** is carefully balanced to mimic the complexities of real-world stock market dynamics and provides a comprehensive resource for advanced time series analysis and forecasting techniques. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"daily_stock_prices_full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the jupyter notebook \"download_stocks_data.ipynb\" in this present directory\n",
    "# to create the following file\n",
    "input_fname = \"original_downloaded_stocks_data.csv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./../../processed/{dataset_name}/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "outp_fname = os.path.join(output_dir, f'{dataset_name}.csv')\n",
    "outp_fig_fname = os.path.join(output_dir, f'{dataset_name}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read selected stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = pd.read_csv(input_fname)\n",
    "stocks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick random date for each ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = stocks['Ticker'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (stocks['Date'] >= '1990-01-01') & (stocks['Date'] <= '2016-01-01')\n",
    "unique_valid_dates = stocks[mask]['Date'].unique()\n",
    "random_dates = pd.Series(unique_valid_dates).sample(len(tickers), replace=True, random_state=42).tolist()\n",
    "# print(random_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get 4 years of daily data for each ticker starting from it's corresponding random date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = []\n",
    "for t, start_date in zip(tickers, random_dates):\n",
    "    subset = stocks[base['Ticker'] == t]\n",
    "    subset = subset[subset['Date'] >= start_date]\n",
    "    subset = subset.iloc[:1000]\n",
    "    stock_df.append(subset)\n",
    "\n",
    "final_data = pd.concat(stock_df)\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify all tickers have same number of rows\n",
    "final_data['Ticker'].value_counts().nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anonymize Stock Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "unique_tickers = final_data['Ticker'].unique()\n",
    "\n",
    "# Generate a random string of fixed length, say 5 characters\n",
    "def generate_random_string(length=8):\n",
    "    return ''.join(random.choices(string.ascii_uppercase, k=length))\n",
    "\n",
    "# Create a mapping dictionary from original tickers to random strings\n",
    "ticker_mapping = {ticker: generate_random_string() for ticker in unique_tickers}\n",
    "\n",
    "# Ensure uniqueness of the random strings, if not regenerate\n",
    "while len(set(ticker_mapping.values())) < len(ticker_mapping):\n",
    "    for ticker in ticker_mapping:\n",
    "        ticker_mapping[ticker] = generate_random_string()\n",
    "\n",
    "# Create a column with masked tickers \n",
    "final_data['Masked_Ticker'] = final_data['Ticker'].map(ticker_mapping)\n",
    "\n",
    "print(final_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Fields in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_col = \"Masked_Ticker\"\n",
    "epoch_col = 'Day_Num'\n",
    "time_col='Date'\n",
    "value_col = 'Adj Close'\n",
    "exog_cols=['Close', 'High', 'Low', 'Open', 'Volume']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the Time Field (if Missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While we do have the 'Date' field, note that we chose different dates per stock\n",
    "# They are not the same dates. We want to treat the time as being the same. \n",
    "# So we will create a new field called 'Day_Num' and make it integer type. \n",
    "# It will start at 1 and increment per day of data. So all stock tickers will\n",
    "# go 1 to 1000 under this field. \n",
    "if epoch_col not in final_data.columns:\n",
    "    final_data[epoch_col]=-1\n",
    "    unique_series = final_data[series_col].unique().tolist()\n",
    "    for s in unique_series:\n",
    "        idx = final_data[series_col] == s\n",
    "        final_data.loc[idx, epoch_col] = np.arange(sum(idx)) + 1\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = [series_col, epoch_col, value_col] + exog_cols    \n",
    "final_data.sort_values(by=[series_col, epoch_col], inplace=True)\n",
    "final_data[all_cols].to_csv(outp_fname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data[all_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
